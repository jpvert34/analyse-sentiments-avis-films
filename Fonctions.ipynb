{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jean Philippe Vert, Franck Ferreboeuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions\n",
    "---\n",
    "Ce fichier contient toutes les fonctions utilisées et appelées dans le fichier main (pour les pré-traitements, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, BiclusterMixin, ClusterMixin, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, fbeta_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.svm.libsvm import cross_validation, decision_function, fit, predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer as DT\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pré-traitements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words associés au domaine du cinéma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_domaine = [\"movie\",\"film\",\"actor\",\"director\",\"production\",\"scenario\",\"casting\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppresion des stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_stopwords(avis, domaine=False): #indique si on utilise les stopwords du domaine\n",
    "    sans_ponctuation = re.sub(\"[^a-zA-Z]\", \" \", avis)\n",
    "    liste_mots = sans_ponctuation.lower().split() #permet de mettre en minuscules et séparer tous les mots\n",
    "    if domaine:\n",
    "        stopw = set(stopwords.words(\"english\") + stopwords_domaine)\n",
    "    else :\n",
    "        stopw = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    liste_mots_sans_stopw = [w for w in liste_mots if not w in stopw] #conserve uniquement les mots non stopwords\n",
    "    \n",
    "    return( \" \".join(liste_mots_sans_stopw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppresion de la ponctuation, tags html..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_ponctuation(dataset, to_lower=False):\n",
    "    liste_mots = []\n",
    "    for i in dataset:\n",
    "        i = re.sub(\"\\S*@\\S*\\s\", \"\", i) # Supprimer les emails\n",
    "        i = re.sub(\"<(br|html|body|tr|td|th|table|div|span|a).*>\", \"\", i) # Supprimer les tags html\n",
    "        i = re.sub(\"[^a-zA-Z]\", \" \", i) # Supprimer ce qui n'est pas des lettres\n",
    "        if to_lower:\n",
    "            i = i.lower()\n",
    "        liste_mots.append(i)\n",
    "    return liste_mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinaisons de prétraitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def pretraitement_combine(avis, supprime_stopwords = False, domaine = True, stem_words = False):\n",
    "    # Effectue plusieurs prétraitements (stopwords, stem words...)\n",
    "\n",
    "    # Supprime le code HTML\n",
    "    liste_mots = BeautifulSoup(avis, \"lxml\").get_text()\n",
    "    \n",
    "    # Met en minuscules et séparent les mots\n",
    "    liste_mots = liste_mots.lower()\n",
    "\n",
    "    # Corrige les mauvaises contractions\n",
    "    liste_mots = re.sub(r\"[^A-Za-z0-9!?\\'\\`]\", \" \", liste_mots)\n",
    "    liste_mots = re.sub(r\"it's\", \" it is\", liste_mots)\n",
    "    liste_mots = re.sub(r\"that's\", \" that is\", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\'s\", \" 's\", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\'ve\", \" have\", liste_mots)\n",
    "    liste_mots = re.sub(r\"won't\", \" will not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"don't\", \" do not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"daren't\", \" dare not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"mustn't\", \" must not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"weren't\", \" were not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"wasn't\", \" was not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"isn't\", \" is not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"can't\", \" can not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"cannot\", \" can not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"n\\'t\", \" not\", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\'re\", \" are\", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\'d\", \" would\", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\'ll\", \" will\", liste_mots)\n",
    "    liste_mots = re.sub(r\"!\", \" ! \", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\?\", \" ? \", liste_mots)\n",
    "    liste_mots = re.sub(r\"\\s{2,}\", \" \", liste_mots)\n",
    "    \n",
    "    # Supprime les stopwords\n",
    "    if supprime_stopwords:\n",
    "        words = liste_mots.split()\n",
    "        if domaine:\n",
    "            stopw = set(stopwords.words(\"english\") + stopwords_domaine)\n",
    "        else:\n",
    "            stopw = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stopw]\n",
    "        liste_mots = \" \".join(words)\n",
    "    \n",
    "    # Effectue la stemmatisation\n",
    "    if stem_words:\n",
    "        words = liste_mots.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        liste_mots = \" \".join(stemmed_words)\n",
    "    \n",
    "    return liste_mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Appliquer un prétraitement\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette fonction permet d'appliquer un prétraitement à toute une liste d'avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applique_pretraitement(dataset, fonction_pretraitement):\n",
    "    nb_avis = len(dataset)\n",
    "    clean_avis = []\n",
    "\n",
    "    #On applique la fonction choisie à tout le document\n",
    "    print('Début prétraitement')\n",
    "    for i in range(0, nb_avis):\n",
    "        clean_avis.append(fonction_pretraitement(dataset[i]))\n",
    "    print('Fin prétraitement')\n",
    "    return clean_avis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Prétraitement combiné avec POSTagging, lemmatisation...\n",
    "Permet d'appliquer différents prétraitements choisis en paramètres en appelant la fonction `dataset_preprocessing(ds, use_pos_tagger=True, use_lem=False, use_stem=False, use_stopwords=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "# WordNet only cares about 5 parts of speech.\n",
    "# The other parts of speech will be tagged as nouns.\n",
    "\n",
    "part = {\n",
    "    'N' : 'n',\n",
    "    'V' : 'v',\n",
    "    'J' : 'a',\n",
    "    'S' : 's',\n",
    "    'R' : 'r'\n",
    "}\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def convert_tag(penn_tag):\n",
    "    '''\n",
    "    convert_tag() accepts the **first letter** of a Penn part-of-speech tag,\n",
    "    then uses a dict lookup to convert it to the appropriate WordNet tag.\n",
    "    '''\n",
    "    if penn_tag in part.keys():\n",
    "        return part[penn_tag]\n",
    "    else:\n",
    "        # other parts of speech will be tagged as nouns\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "#Retourne pour un document unique ses PosTag / Lem / Stem\n",
    "def document_preprocessing(element, use_pos_tagger=True, use_lem=False, use_stem=False, use_stopwords=False):\n",
    "    # list of tuples [('token', 'tag'), ('token2', 'tag2')...]\n",
    "    preprocessed = element.split(\" \")\n",
    "    if use_pos_tagger:\n",
    "        preprocessed = pos_tag(nltk.word_tokenize(element)) # must tag in context\n",
    "    else:\n",
    "        preprocessed = [tuple([i]) for i in preprocessed]\n",
    "    if use_lem:\n",
    "        preprocessed = [preprocessed[k] + (wnl.lemmatize(preprocessed[k][0], convert_tag(preprocessed[k][1][0])), ) for k in range(len(preprocessed))]\n",
    "    if use_stem:\n",
    "        preprocessed = [preprocessed[k] + (stemmer.stem(preprocessed[k][0]), ) for k in range(len(preprocessed))]\n",
    "    if use_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        preprocessed = [w for w in preprocessed if not w[0].lower() in stops]\n",
    "    return preprocessed\n",
    "\n",
    "def dataset_preprocessing(ds, use_pos_tagger=True, use_lem=True, use_stem=True, use_stopwords=True):\n",
    "    print(\"Application de tous les prétraitements\")\n",
    "    new_ds = []\n",
    "    waiting_text = \"Preprocessing, \"\n",
    "    if use_pos_tagger:\n",
    "        waiting_text += \"POS Tagging, \"\n",
    "    if use_lem:\n",
    "        waiting_text += \"Lemmatization, \"\n",
    "    if use_stem:\n",
    "        waiting_text += \"Stemming, \"\n",
    "    if use_stopwords:\n",
    "        waiting_text += \"Removing Stopwords, \"\n",
    "    for i in range(0, len(ds)):\n",
    "        if (i+1) % 500  == 0:\n",
    "            print(waiting_text[:-2] + \" de l'avis {:d} sur {:d}\".format(i+1, len(ds)))\n",
    "        new_ds.append(document_preprocessing(ds[i], use_pos_tagger, use_lem, use_stem, use_stopwords))\n",
    "    return new_ds\n",
    "\n",
    "# Crée un dataset traitable par les classifieurs sans les pos_tag non pertinents et lemmatisation/stemming\n",
    "def create_text_dataset_without_postags(dataset_with_postags, remove_tags=[], transform_to_lem=True, transform_to_stem=True):\n",
    "    datasize = len(dataset_with_postags[0][0])\n",
    "    if datasize < 2:\n",
    "        print(\"Warning: Votre dataset ne possède pas les données appropriées\")\n",
    "        return []\n",
    "    dataset_without_postags = []\n",
    "    for i in dataset_with_postags:\n",
    "        removed_postag = [j for j in i if not j[1] in remove_tags]\n",
    "        li = []\n",
    "        if transform_to_lem and transform_to_stem:\n",
    "            #Si le mot a été lemmatisé, on le prend, sinon on prend le mot avec stemming appliqué\n",
    "            for k in removed_postag:\n",
    "                if k[2] != k[0]:\n",
    "                    li.append(k[2])\n",
    "                else:\n",
    "                    li.append(k[3])\n",
    "        elif transform_to_lem and datasize >= 3:\n",
    "            for k in removed_postag:\n",
    "                li.append(k[2])\n",
    "        elif transform_to_stem and datasize >= 4:\n",
    "            for k in removed_postag:\n",
    "                li.append(k[3])\n",
    "        else:\n",
    "            for k in removed_postag:\n",
    "                li.append(k[0])\n",
    "        dataset_without_postags.append(\" \".join(li))\n",
    "    return dataset_without_postags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Récuperer le nom de la variable d'un classfieur (pour sauvegarder le classifieur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def retrieve_name(var):\n",
    "    for fi in reversed(inspect.stack()):\n",
    "        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "        if len(names) > 0:\n",
    "            return names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualisation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud (nuage de mots utilisé pour les stopwords...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_draw(data, color = 'black', maskPath=\"\", width=2500, height=2000, filename=\"train\"):\n",
    "    mask = \"\"\n",
    "    if maskPath != \"\":\n",
    "        mask = np.array(Image.open(maskPath))\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "    if maskPath != \"\":\n",
    "        wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=width,\n",
    "                      height=height,\n",
    "                          mask=mask\n",
    "                     ).generate(cleaned_word)\n",
    "    else:\n",
    "        wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=width,\n",
    "                      height=height,\n",
    "                     ).generate(cleaned_word)\n",
    "    fig = plt.figure(1, figsize=(8.27, 11.69), dpi=250)\n",
    "    filename = color + '-' + filename + '.png'\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"./visualisation/\" + filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature(data_set, labels_set):\n",
    "    # create an instance for tree feature selection\n",
    "    tree_clf = ExtraTreesClassifier()\n",
    "\n",
    "    # first create arrays holding input and output data\n",
    "\n",
    "    # Vectorizing Train set\n",
    "    cv = TfidfVectorizer(analyzer='word')\n",
    "    x_train = cv.fit_transform(data_set)\n",
    "\n",
    "    # Creating an object for Label Encoder and fitting on target strings\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels_set)\n",
    "\n",
    "    # fit the model\n",
    "    tree_clf.fit(x_train, y)\n",
    "\n",
    "    # Preparing variables\n",
    "    importances = tree_clf.feature_importances_\n",
    "    feature_names = cv.get_feature_names()\n",
    "    feature_imp_dict = dict(zip(feature_names, importances))\n",
    "    sorted_features = sorted(feature_imp_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    for f in range(20):\n",
    "        print(\"feature %d : %s (%f)\" % (indices[f], sorted_features[f][0], sorted_features[f][1]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(100), importances[indices[:100]],\n",
    "           color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(100), sorted_features[:100], rotation=90)\n",
    "    plt.xlim([-1, 100])\n",
    "    plt.show()\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipelines\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listes des pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_pipelines = [\n",
    "    [('tfvect', TfidfVectorizer()), \n",
    "     ('tft', TfidfTransformer()), \n",
    "     ('mnb', MultinomialNB())],\n",
    "    [('cvect', CountVectorizer()), \n",
    "     ('tft', TfidfTransformer()), \n",
    "     ('mnb', MultinomialNB())],\n",
    "    [('tfvect', TfidfVectorizer()), \n",
    "     ('reduce_dim', TruncatedSVD()), \n",
    "     ('gnb', GaussianNB())],\n",
    "    [('tfvect', TfidfVectorizer()), \n",
    "     ('tft', TfidfTransformer()),\n",
    "     ('reduce_dim', TruncatedSVD()), \n",
    "     ('gnb', GaussianNB())],\n",
    "    [('tfvect', TfidfVectorizer()),\n",
    "     ('knc', KNeighborsClassifier()),],\n",
    "    [('tfvect', TfidfVectorizer()), \n",
    "     ('tft', TfidfTransformer()), \n",
    "     ('sgdc', SGDClassifier())]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramètres des pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parameters = [\n",
    "    {\n",
    "        \"tft__use_idf\": (True, False),\n",
    "        \"tfvect__ngram_range\": [(1,1),(1,2),(1,3)],\n",
    "        \"tfvect__smooth_idf\": (True, False),\n",
    "        \"tfvect__sublinear_tf\": (True, False),\n",
    "        \n",
    "    }, \n",
    "    {\n",
    "        \"tft__use_idf\": (True, False),\n",
    "        \"cvect__ngram_range\": [(1,1),(1,2), (1,3)],\n",
    "    },\n",
    "    {\n",
    "        \"reduce_dim__n_components\":(5,),\n",
    "        \"reduce_dim__n_iter\": (7,),\n",
    "        \"reduce_dim__random_state\" : (42,),\n",
    "    }, \n",
    "    {\n",
    "        \"reduce_dim__n_components\":(5,),\n",
    "        \"reduce_dim__n_iter\": (7,),\n",
    "        \"reduce_dim__random_state\" : (42,),\n",
    "        \"tft__use_idf\": (True, False),\n",
    "    }, \n",
    "    {\n",
    "        \"knc__n_neighbors\": (2,3,5),\n",
    "        \"knc__weights\": (\"uniform\", \"distance\"),\n",
    "        \"knc__metric\": (\"euclidean\", \"minkowski\"),\n",
    "        \n",
    "    }, \n",
    "    {\n",
    "        \"tft__use_idf\": (True, False),\n",
    "        \"sgdc__max_iter\": (500,1000),\n",
    "        \"sgdc__alpha\": (0.001, 0.0001, 0.00001),\n",
    "    }, \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Résultats du modèle\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afficher les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(pred, pred_proba, y):    \n",
    "    roc_auc = roc_auc_score(y, pred_proba)\n",
    "    avr_prec = average_precision_score(y, pred_proba)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    f1 = f1_score(y, pred)\n",
    "    fbeta_05 = fbeta_score(y, pred, 0.5)\n",
    "    fbeta_2 = fbeta_score(y, pred, 2.0)\n",
    "    prec = precision_score(y, pred)\n",
    "    rec = recall_score(y, pred)\n",
    "    conf = confusion_matrix(y, pred)\n",
    "    \n",
    "    result = {\n",
    "        'roc_auc': roc_auc*100, \n",
    "        'average_precision': avr_prec*100,\n",
    "        'f1': f1*100, \n",
    "        'f05': fbeta_05*100,\n",
    "        'f2': fbeta_2*100,\n",
    "        'acc': acc*100, \n",
    "        'precision': prec*100, \n",
    "        'recall': rec*100, \n",
    "        'confusion_matrix': conf\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def prettify_output_results(res):\n",
    "    st = \"\"\n",
    "    st += \"Accuracy : \\t\" + str(res['acc']) + \"\\n\"\n",
    "    st += \"Precision : \\t\" + str(res['precision']) + \"\\n\"\n",
    "    st += \"Recall : \\t\\t\" + str(res['recall']) + \"\\n\"\n",
    "    st += \"F1 : \\t\\t\" + str(res['f1']) + \"\\n\"\n",
    "    st += \"F0.5 : \\t\\t\" + str(res['f05']) + \"\\n\"\n",
    "    st += \"F2 : \\t\\t\" + str(res['f2']) + \"\\n\"\n",
    "    st += \"Average precision: \\t\" + str(res['average_precision']) + \"\\n\"\n",
    "    st += \"ROC AUC score: \\t\\t\" + str(res['roc_auc']) + \"\\n\"\n",
    "    st += \"\\nConfusion Matrix : \\n\" + str(res['confusion_matrix']) + \"\\n\"\n",
    "    return st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrer les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_disk(model, results):\n",
    "    choix = input(\"Voulez vous sauvegarder les résultats ? (y/n) \")\n",
    "    if 'y' not in choix:\n",
    "        print(\"Pas de sauvegarde\")\n",
    "    else:\n",
    "        import os\n",
    "        def numbers( path ):\n",
    "            for filename in os.listdir(path):\n",
    "                name = filename.split(\".\")\n",
    "                yield int(name[0][6:])\n",
    "        count = max( numbers( './results_report/' ) )\n",
    "        \n",
    "        model_txt = \"\\n\".join([\" \".join(re.split(\"\\s+\", str(i))) for i in model.steps])\n",
    "        pretraitements = input(\"Quels sont les prétraitements sur les données?\")\n",
    "        remarques = input(\"Avez vous des commentaires ?\")\n",
    "        count += 1\n",
    "        filename = \"./results_report/report\" + str(count) + \".txt\"\n",
    "        report = open(filename, \"w\")\n",
    "        report.write(\"Prétraitements: \" + pretraitements + \"\\n\\n\")\n",
    "        report.write(\"Notes : \"+ remarques + \"\\n\\n\")\n",
    "        report.write(model_txt+\"\\n\\n\")\n",
    "        report.write(prettify_output_results(results))\n",
    "        report.close()\n",
    "        print(\"Report created as report\" + str(count) + \".txt for model with \" + str(results[\"acc\"]*100) + \"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
